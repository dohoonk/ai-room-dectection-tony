# Location Detection AI — Product Requirements Document (PRD)

**Last Updated:** 2025-11-09  
**Version:** 2.0 (Updated with AWS AI/ML Services Strategy)

## 1. Overview

### 1.1 Goal
Enable automatic detection of **room boundaries** in architectural floorplans. The system should identify closed interior spaces (rooms) from a set of wall line segments and return normalized bounding boxes that can be displayed in the UI.

### 1.2 MVP Scope
For the MVP, the blueprint’s wall geometry will be **provided explicitly** as a JSON list of normalized line segments. The system will **not** perform PDF parsing, raster wall detection, or OCR in this phase.

This allows us to focus on the **core spatial reasoning** component:
> Detecting closed shapes (rooms) from wall line connections.

## 2. Problem Statement

Users currently draw room boundaries manually by clicking along wall edges. This is:
- Time-consuming
- Error-prone
- A poor onboarding experience

Automating boundary detection:
- Reduces blueprint setup time by **80–95%**
- Increases confidence and consistency
- Makes demos dramatically more compelling

## 3. Success Criteria

| Metric | Target |
|-------|--------|
| Detection accuracy | ≥ 90% correct rooms on clean inputs |
| False positives | < 10% incorrect room detections |
| Processing latency | < **30 seconds** per blueprint |
| User correction effort | Minimal (adjust shapes, not draw from scratch) |

## 4. Input & Output Specification

### 4.1 Input Format (Walls JSON)
Coordinates are normalized to a **0–1000** coordinate system.

```json
[
  {
    "type": "line",
    "start": [100, 100],
    "end": [500, 100],
    "is_load_bearing": false
  },
  {
    "type": "line",
    "start": [100, 100],
    "end": [100, 400],
    "is_load_bearing": false
  }
]
```

A **raster blueprint image** (PNG/JPG) is also provided for visual display only.

### 4.2 Output Format (Detected Rooms)

```json
[
  {
    "id": "room_001",
    "bounding_box": [50, 50, 200, 300],
    "name_hint": "Entry Hall"
  },
  {
    "id": "room_002",
    "bounding_box": [250, 50, 700, 500],
    "name_hint": "Main Office"
  }
]
```

Bounding boxes remain axis-aligned for MVP simplicity.

## 5. System Flow (High-Level)

### 5.1 MVP (Local Prototype)
User → React Frontend (Material UI) → Uploads (image + wall-lines JSON)
React → Local Backend API (Express/FastAPI)
Backend → Detects closed polygon spaces (rooms) using Python `networkx` + `shapely`
Backend → Returns bounding box list (normalized 0–1000)
React → Draws boundary overlays on the image using Material UI components

### 5.2 Phase 2 (AWS Infrastructure - Post-MVP)
User → React Frontend → Uploads (image + wall-lines JSON)
React → AWS Location Detection Service
Service → Detects closed polygon spaces (rooms)
Service → Returns bounding box list (normalized 0–1000)
React → Draws boundary overlays on the image

## 6. Technical Architecture

### 6.1 MVP (Local Prototype)
| Component | Purpose | Technology |
|---|---|---|
| React Frontend | Renders blueprint and overlays detected rooms | React + Material UI |
| Local Backend API | REST API for room detection | Express.js or FastAPI |
| Core Algorithm | Face-finding algorithm for multi-room detection | Python: `networkx`, `shapely` (polygonize) |
| Sample Test Data | Blueprint images and wall-line JSON for testing | JSON files + PNG/JPG images |

**Development Approach**: Build local prototype first. Each backend task must have a corresponding React UI task for manual testing as we go.

**UI Library**: Material UI (MUI) for all React components.

### 6.2 Phase 2 (AWS Infrastructure - Post-MVP)
| Component | Purpose |
|---|---|
| React Frontend | Renders blueprint and overlays detected rooms |
| S3 | Stores input + generated room JSON |
| API Gateway + Lambda | Job creation + status API |
| DynamoDB | Job state and metadata |
| SQS Queue | Decoupled room-detection job processing |
| ECS Fargate Worker | Runs room detection graph algorithm |
| Python: `networkx`, `shapely` | Geometry + face-finding algorithm (polygonize) for multi-room detection |

Latency target: **< 30s** from job submit → response availability.

## 7. Core Algorithm Requirements

1. Parse line segments
2. Snap endpoints within tolerance to form a wall adjacency graph
3. **Detect all bounded regions (faces)** using face-finding algorithm:
   - Split wall segments at intersection points
   - Use Shapely's polygonize to find all polygons formed by the line segments
   - This enables detection of all rooms, including those formed by internal walls and T-junctions
   - Fallback to graph-based cycle detection if polygonize doesn't find regions
4. Filter cycles/faces by size/shape (ignore closets, hall fragments, invalid polygons)
5. Convert polygon → bounding box
6. Normalize bounding box into 0–1000 coordinate space

## 8. Demo & UX Differentiation Requirements

### 8.1 Observability (must display in UI)
```
Rooms Detected: N
Processing Time: X.XX seconds
Confidence Score: 0.00–1.00
```

### 8.2 Human-in-the-Loop UX
- User can click a detected room to highlight it
- Optional: user can rename room
- Optional: user can remove incorrect detection

### 8.3 Product Value Story (Demo Script Required)
> “Manual tracing takes 5–15 minutes of clicking.  
> With Location Detection AI, rooms appear automatically in under 3 seconds.  
> The user now **reviews**, not **draws**.”

### 8.4 Before / After Comparison Slide (Required)
| Manual Workflow | With Detection AI |
|---|---|
| 40–100 clicks | Zero drawing |
| Requires CAD skill | No training needed |
| 5–15 min setup | < 5 sec |
| Inconsistent result | Deterministic + repeatable |

## 9. Phase 2 (Post-MVP Roadmap)

| Phase | Addition | Description |
|---|---|---|
| 2A | PDF Vector Extraction | Automatically detect wall lines from PDFs using PyMuPDF + AWS services |
| 2B | Raster Wall Detection | OpenCV-based wall parsing for scanned plans + AWS services |
| 2C | Room Label OCR | Textract → auto populate `name_hint` |
| 2D | Polygon Output | Replace bounding boxes with true room polygons |

### 9.0 AWS AI/ML Services Integration Strategy

**Decision:** Use pre-built AWS AI/ML services (Textract, Rekognition) combined with our proven algorithms, rather than training custom SageMaker models.

**Chosen Approach (Strategy 1):**
```
Image/PDF → S3
  ↓
AWS Services (Pre-Built):
  ├─ Textract → Extract text (room labels, dimensions) ✅ Pre-built
  ├─ Rekognition → Detect objects (doors, windows) ✅ Pre-built
  └─ Our Algorithms → Extract wall lines (PyMuPDF/OpenCV)
  ↓
Our Algorithm (NetworkX + Shapely) → Detect rooms
```

**Rationale:**
1. **Compliance:** Uses AWS AI/ML Services (Textract, Rekognition) as required
2. **Similar to DocumentAI:** DocumentAI uses pre-built services, not custom models
3. **No Training Needed:** Faster development (2-3 weeks vs 8-14 weeks)
4. **Lower Cost:** ~$16-56 per 1000 requests vs $20-60+ with training costs
5. **Proven Performance:** Our algorithms already achieve 100% accuracy
6. **Flexibility:** Can add SageMaker later if needed

**Tradeoffs Considered:**

| Approach | Pros | Cons | Decision |
|----------|------|------|----------|
| **Pre-Built Services Only** | Fast, low cost, compliant, proven | Uses our algorithms | ✅ **Chosen** |
| **Full SageMaker Training** | Maximum AWS compliance | Slow (8-14 weeks), expensive, complex | ❌ Rejected |
| **Minimal SageMaker** | Shows SageMaker usage | Adds complexity without value | ❌ Rejected |

**AWS Services Used:**
- **Amazon Textract:** OCR for room labels and dimensions (Phase 2C)
- **Amazon Rekognition:** Object detection for doors, windows, stairs (optional enhancement)
- **Amazon S3:** File storage (required for AWS services)
- **Our Algorithms:** Core processing (PyMuPDF, OpenCV, NetworkX, Shapely)

**Compliance Statement:**
This approach satisfies the requirement for "AWS AI/ML Services" by using Textract and Rekognition for document processing capabilities, similar to how DocumentAI provides pre-built services. SageMaker remains available for future enhancements if custom model training becomes necessary.

## 9A. PDF Vector Extraction (Phase 2A)

### 9A.1 Goal
Enable automatic extraction of wall line segments from PDF blueprint files, eliminating the need for manual JSON input. The system should parse PDF vector graphics and convert them to the existing wall segment format.

### 9A.2 Technical Approach

**Input**: PDF file (architectural blueprint)
**Output**: Wall segments in JSON format (compatible with existing `/detect-rooms` endpoint)

**Processing Pipeline**:
1. **File Upload**: Upload PDF to S3 (AWS requirement)
2. **PDF Parsing**: Extract vector paths and line segments using PyMuPDF/pdfplumber
3. **AWS Services (Parallel)**:
   - **Textract**: Extract text labels and dimensions (optional, for Phase 2C)
   - **Rekognition**: Detect objects like doors, windows (optional enhancement)
4. **Coordinate Transformation**: Map PDF coordinate system to normalized 0-1000 range
5. **Line Filtering**: Identify wall-like elements (thickness, style, layer)
6. **Segment Conversion**: Convert PDF paths to wall segment format
7. **Validation**: Ensure extracted segments form valid wall connections
8. **Room Detection**: Use existing algorithm (NetworkX + Shapely) to detect rooms

### 9A.3 Technology Stack

| Component | Technology | Purpose |
|---|---|---|
| File Storage | Amazon S3 | Store PDF files (AWS requirement) |
| PDF Parser | `PyMuPDF` (fitz) or `pdfplumber` | Extract vector paths and line segments |
| OCR Service | Amazon Textract | Extract text labels and dimensions (optional) |
| Object Detection | Amazon Rekognition | Detect architectural elements (optional) |
| Geometry Processing | `shapely` | Process and validate extracted lines |
| Coordinate Transformation | Custom transformation logic | Normalize PDF coordinates to 0-1000 range |

### 9A.4 Implementation Details

**PDF Parsing Strategy**:
- Extract all vector paths from PDF pages
- Filter paths by:
  - Line thickness (walls typically thicker than annotations)
  - Color/style (walls often black or specific color)
  - Layer name (if available in PDF structure)
- Convert paths to line segments (start/end coordinates)

**Coordinate System Handling**:
- Detect PDF page dimensions
- Map PDF coordinates to normalized 0-1000 system
- Preserve aspect ratio during transformation
- Handle rotated or scaled PDFs

**Line Segment Extraction**:
- Identify wall-like elements:
  - Thickness > threshold (e.g., 2-5 pixels)
  - Straight lines (not curves or complex paths)
  - Connected endpoints (form closed regions)
- Filter out annotations, dimensions, text, symbols

**Validation**:
- Ensure extracted segments form connected graph
- Verify minimum segment length
- Check for reasonable wall density

### 9A.5 API Endpoint

**New Endpoint**: `POST /detect-rooms-from-pdf`

**Request**:
- `multipart/form-data` with PDF file
- Optional parameters:
  - `min_line_thickness`: Minimum wall thickness (default: 2.0)
  - `coordinate_tolerance`: Endpoint snapping tolerance (default: 1.0)

**Response**:
- Same format as `/detect-rooms` (array of room objects)
- Includes metadata:
  - `extraction_method`: "pdf_vector"
  - `segments_extracted`: Number of wall segments found
  - `processing_time`: Time for PDF parsing + room detection

### 9A.6 Success Criteria

| Metric | Target |
|--------|--------|
| PDF parsing accuracy | ≥ 85% of walls correctly extracted |
| Coordinate accuracy | < 2% coordinate transformation error |
| Processing latency | < 10 seconds for typical PDF (1-5 pages) |
| False positives | < 15% non-wall lines incorrectly identified as walls |

### 9A.7 Testing Strategy

- Test with various PDF types:
  - CAD-generated PDFs (vector-based)
  - Scanned PDFs (raster embedded in PDF)
  - Multi-page blueprints
  - Different coordinate systems
- Compare extracted segments with ground truth JSON
- Validate room detection accuracy matches JSON input method

## 9B. Raster Image Processing (Phase 2B)

### 9B.1 Goal
Enable automatic extraction of wall line segments from raster images (PNG, JPG, JPEG), supporting scanned blueprints and photographed floorplans. The system should use computer vision techniques to detect wall lines.

### 9B.2 Technical Approach

**Input**: Raster image file (PNG, JPG, JPEG)
**Output**: Wall segments in JSON format (compatible with existing `/detect-rooms` endpoint)

**Processing Pipeline**:
1. **File Upload**: Upload image to S3 (AWS requirement)
2. **Image Preprocessing**: Enhance image quality, remove noise using OpenCV
3. **AWS Services (Parallel)**:
   - **Textract**: Extract text labels and dimensions (optional, for Phase 2C)
   - **Rekognition**: Detect objects like doors, windows, stairs (optional enhancement)
4. **Edge Detection**: Identify edges using Canny algorithm (OpenCV)
5. **Line Detection**: Extract straight lines using Hough transforms (OpenCV)
6. **Line Filtering**: Filter wall-like lines (length, orientation, connectivity)
   - Use Rekognition results to filter out non-wall elements
7. **Segment Conversion**: Convert detected lines to wall segment format
8. **Validation**: Ensure segments form valid wall connections
9. **Room Detection**: Use existing algorithm (NetworkX + Shapely) to detect rooms

### 9B.3 Technology Stack

| Component | Technology | Purpose |
|---|---|---|
| File Storage | Amazon S3 | Store image files (AWS requirement) |
| Image Processing | `opencv-python` (cv2) | Image loading, preprocessing, edge detection |
| Line Detection | `opencv-python` (Hough transforms) | Detect straight lines in images |
| OCR Service | Amazon Textract | Extract text labels and dimensions (optional) |
| Object Detection | Amazon Rekognition | Detect architectural elements (optional) |
| Geometry Processing | `shapely`, `numpy` | Process and validate detected lines |
| Coordinate Transformation | Custom transformation logic | Normalize image coordinates to 0-1000 range |

### 9B.4 Implementation Details

**Image Preprocessing**:
- Convert to grayscale
- Apply noise reduction (Gaussian blur, median filter)
- Enhance contrast (histogram equalization, adaptive thresholding)
- Handle different image orientations (rotation correction)

**Edge Detection**:
- Apply Canny edge detector with adaptive thresholds
- Parameters:
  - `low_threshold`: Lower bound for edge detection
  - `high_threshold`: Upper bound for edge detection
  - Adjust based on image quality

**Line Detection**:
- Use Probabilistic Hough Line Transform (`cv2.HoughLinesP`)
- Parameters:
  - `rho`: Distance resolution (1 pixel)
  - `theta`: Angular resolution (1 degree)
  - `threshold`: Minimum votes for line detection
  - `min_line_length`: Minimum line length (filter short segments)
  - `max_line_gap`: Maximum gap between line segments to connect

**Line Filtering**:
- Filter by length (walls typically longer than annotations)
- Filter by orientation (prefer horizontal/vertical, but allow angles)
- Group nearby parallel lines (merge close lines)
- Remove duplicate or overlapping lines

**Coordinate System Handling**:
- Detect image dimensions
- Map image pixel coordinates to normalized 0-1000 system
- Preserve aspect ratio during transformation
- Handle different image resolutions

**Segment Conversion**:
- Convert detected lines to wall segments:
  - Start/end coordinates
  - Calculate approximate thickness (if detectable)
  - Set `is_load_bearing` to false (not detectable from raster)

**Validation**:
- Ensure extracted segments form connected graph
- Verify minimum segment count
- Check for reasonable wall density

### 9B.5 API Endpoint

**New Endpoint**: `POST /detect-rooms-from-image`

**Request**:
- `multipart/form-data` with image file (PNG, JPG, JPEG)
- Optional parameters:
  - `canny_low`: Canny edge detection low threshold (default: 50)
  - `canny_high`: Canny edge detection high threshold (default: 150)
  - `hough_threshold`: Hough line detection threshold (default: 100)
  - `min_line_length`: Minimum line length in pixels (default: 50)
  - `coordinate_tolerance`: Endpoint snapping tolerance (default: 1.0)

**Response**:
- Same format as `/detect-rooms` (array of room objects)
- Includes metadata:
  - `extraction_method`: "raster_image"
  - `segments_extracted`: Number of wall segments found
  - `processing_time`: Time for image processing + room detection
  - `image_dimensions`: Original image width/height

### 9B.6 Success Criteria

| Metric | Target |
|--------|--------|
| Line detection accuracy | ≥ 75% of walls correctly detected |
| False positive rate | < 25% non-wall lines incorrectly identified |
| Processing latency | < 15 seconds for typical image (2000x3000 pixels) |
| Image quality tolerance | Works with images ≥ 150 DPI |

### 9B.7 Testing Strategy

- Test with various image types:
  - High-resolution scanned blueprints
  - Low-resolution photographs
  - Different lighting conditions
  - Various architectural styles
  - Hand-drawn vs CAD-generated plans
- Compare detected segments with ground truth JSON
- Validate room detection accuracy matches JSON input method
- Test parameter tuning for different image qualities

### 9B.8 Challenges & Mitigation

**Challenge**: Variable image quality
- **Mitigation**: Adaptive thresholding, multiple preprocessing strategies

**Challenge**: Noise and artifacts
- **Mitigation**: Noise reduction filters, line length filtering

**Challenge**: Perspective distortion
- **Mitigation**: Perspective correction (if detectable), tolerance for non-perfect lines

**Challenge**: Different drawing styles
- **Mitigation**: Parameter tuning, user-configurable thresholds

**Challenge**: Text and annotations
- **Mitigation**: Filter short lines, prefer longer continuous lines

## 10. Deliverables

| Deliverable | Description |
|---|---|
| `detect_rooms()` core implementation | Graph-based polygon + bounding box detection (Python) |
| Local Backend API | REST API endpoint for room detection (Express/FastAPI) |
| React UI with Material UI | Display detected room boundaries interactively |
| Sample Test Data | Blueprint images and wall-line JSON files for testing |
| Demo video | 1–3 minutes showing workflow & review UX |
| README | Includes value story, architecture, and roadmap |

## 11. Development Workflow Requirements

### 11.1 Task Pairing
- **Every backend task must have a corresponding React UI task**
- Backend and UI tasks should be developed in parallel for manual testing
- UI tasks should use Material UI components consistently

### 11.2 Testing Strategy
- Build sample test data (blueprint images + wall-line JSON) early
- Manual testing after each backend/UI task pair completion
- Test with progressively complex floorplans

### 11.3 Priority Order
1. Core `detect_rooms()` algorithm (Python)
2. Sample test data creation
3. Local backend API endpoint
4. React UI with Material UI for visualization
5. AWS infrastructure (Phase 2)
